{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL_list = [\"https://mb.com.ph/2021/03/11/\",\n",
    "            \"https://mb.com.ph/2021/03/11/page/2/\",\n",
    "            \"https://mb.com.ph/2021/03/11/page/3/\",\n",
    "            \"https://mb.com.ph/2021/03/11/page/4/\",\n",
    "            \"https://mb.com.ph/2021/03/11/page/5/\",\n",
    "            \"https://mb.com.ph/2021/03/11/page/6/\",\n",
    "            \"https://mb.com.ph/2021/03/11/page/7/\",\n",
    "            \"https://mb.com.ph/2021/03/11/page/8/\",\n",
    "            \"https://mb.com.ph/2021/03/11/page/9/\",\n",
    "            \"https://mb.com.ph/2021/03/11/page/10/\",\n",
    "            \"https://mb.com.ph/2021/03/11/page/11/\",\n",
    "            \"https://mb.com.ph/2021/03/11/page/12/\",\n",
    "            \"https://mb.com.ph/2021/03/11/page/13/\",\n",
    "            \"https://mb.com.ph/2021/03/11/page/14/\",\n",
    "            \"https://mb.com.ph/2021/03/12/\",\n",
    "            \"https://mb.com.ph/2021/03/12/page/2/\",\n",
    "            \"https://mb.com.ph/2021/03/12/page/3/\",\n",
    "            \"https://mb.com.ph/2021/03/12/page/4/\",\n",
    "            \"https://mb.com.ph/2021/03/12/page/5/\",\n",
    "            \"https://mb.com.ph/2021/03/12/page/6/\",\n",
    "            \"https://mb.com.ph/2021/03/12/page/7/\",\n",
    "            \"https://mb.com.ph/2021/03/12/page/8/\",\n",
    "            \"https://mb.com.ph/2021/03/12/page/9/\",\n",
    "            \"https://mb.com.ph/2021/03/12/page/10/\",\n",
    "            \"https://mb.com.ph/2021/03/12/page/11/\",\n",
    "            \"https://mb.com.ph/2021/03/12/page/12/\",\n",
    "            \"https://mb.com.ph/2021/03/12/page/13/\",\n",
    "            \"https://mb.com.ph/2021/03/12/page/14/\",\n",
    "            \"https://mb.com.ph/2021/03/12/page/15/\"\n",
    "           ]\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36 Edg/89.0.774.50'}\n",
    "\n",
    "article_data = []\n",
    "\n",
    "for list_num in range(len(URL_list)):\n",
    "    page = requests.get(URL_list[list_num], headers=headers)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    #Get list of articles\n",
    "    art_list = soup.find(id='news-feed')\n",
    "    art_link = art_list.find_all('h4')\n",
    "    \n",
    "    #Get article links\n",
    "    art_url = []\n",
    "    for art in range(len(art_link)):\n",
    "        for link in art_link[art].find_all('a'):\n",
    "                art_url.append(link.get('href'))\n",
    "                \n",
    "    if len(art_url) >= 18:\n",
    "        art_range = 18\n",
    "    elif len(art_url) < 18:\n",
    "        art_range = len(art_url)\n",
    "\n",
    "    for url in range(art_range):\n",
    "        page = requests.get(art_url[url], headers=headers)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        # Get main content <article>\n",
    "        art_content=soup.find(id='the-article-content')\n",
    "\n",
    "        # Get all needed information\n",
    "        if list_num == 28 and url == 6:\n",
    "            art_content=soup.find(id='article-title')\n",
    "            title = art_content.find_all('h1')\n",
    "            content = art_content.find_all('span')\n",
    "            date = content[2].text.split(\", \")\n",
    "            art_author = content[0].text.strip('by ')\n",
    "        else:\n",
    "            title = art_content.find_all('h2')\n",
    "            content = art_content.find_all('p')\n",
    "            date = content[0].text.strip('Published ').split(\", \")\n",
    "            art_author = content[1].text.strip('by ')\n",
    "        \n",
    "        full_article = \"\"\n",
    "        for text in range(2, len(content), 1):\n",
    "            if(content[text].text.strip() != \"\"):\n",
    "                paragraph = content[text].text.strip()\n",
    "                full_article = full_article + \" \" + paragraph\n",
    "        \n",
    "        art_date = date[0] + \", \" + date[1]\n",
    "        art_title = title[0].text.strip()\n",
    "\n",
    "        article_data.append({\n",
    "                \"date\": art_date,\n",
    "                \"title\": art_title,\n",
    "                \"full_article\": full_article,\n",
    "                \"author\": art_author\n",
    "        })\n",
    "        print(\"\\tDone scraping article:\", url)\n",
    "    print(\"Done scraping page:\", list_num)\n",
    "        \n",
    "#generate json file for all articles\n",
    "f = open(\"articles_mb.json\", \"w\", encoding=\"utf-8\")\n",
    "json_str = str(article_data)\n",
    "f.write(json_str)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
